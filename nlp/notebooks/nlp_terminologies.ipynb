{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7064a78e-69c5-49bb-8cd2-f6448311466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0427e9-729b-45a7-b46a-ac47b2742079",
   "metadata": {},
   "source": [
    "### In order to work on text documents, Gensim requires the words (aka tokens) be converted to unique ids. In order to achieve that, Gensim lets you create a Dictionary object that maps each word to a unique id."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2abbc0-f444-442f-8b4c-702525d89334",
   "metadata": {},
   "source": [
    "The dictionary object is typically used to create a ‘bag of words’ Corpus. It is this Dictionary and the bag-of-words (Corpus) that are used as inputs to topic modeling and other models that Gensim specializes in.\n",
    "\n",
    "Alright, what sort of text inputs can gensim handle? The input text typically comes in 3 different forms:\n",
    "\n",
    "- As sentences stored in python’s native list object\n",
    "- As one single text file, small or large.\n",
    "- In multiple text files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a5c49-a451-4e73-a8a3-4bf15d48817b",
   "metadata": {},
   "source": [
    "A ‘token’ typically means a ‘word’. A ‘document’ can typically refer to a ‘sentence’ or ‘paragraph’ and a ‘corpus’ is typically a ‘collection of documents as a bag of words’. That is, for each document, a corpus contains each word’s id and its frequency count in that document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4bf4253-5adb-4ae6-ab69-75ded8e022b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to create a dictionary from a list of sentences?\n",
    "documents_1 = [\"Your childhood teacher did not wrong you when they taught you that there should be three, or four, or five sentences in a paragraph.\" ,\n",
    "             \"It is important to understand, however, that the aim in teaching this was not to impart a hard-and-fast rule of grammar\" ,\n",
    "             \"drawn from an authoritative-but-dusty book.\",\n",
    "             \"The true aim of this strategy was to teach you that your ideas must be well supported to be persuasive and effective.\"]\n",
    "\n",
    "documents_2 = [\"Recent research has provided a wealth of insight about how dogs came to be domesticated by humans and the roles they played in Native American culture.\",\n",
    "               \"DNA studies on archaeological finds suggest that dogs may have been domesticated by humans as long as 40,000 years ago.\",\n",
    "               \"When the first humans came to North America from Eurasia, at least 12,000 years ago, domesticated dogs came with them.\",\n",
    "               \" They appear to have been highly prized by early North American hunter-gatherers and were their only animal companions for centuries\", \n",
    "               \"since there were no horses on the continent until the 16th century.\"]\n",
    "documents = documents_1 + documents_2\n",
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text.lower() for text in doc.split()] for doc in documents]\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bad2bd19-06c0-4915-9097-182974a1c259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(114 unique tokens: ['a', 'be', 'childhood', 'did', 'five']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a6484af-4481-4f1a-90aa-dd5714bc5142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'be': 1, 'childhood': 2, 'did': 3, 'five': 4, 'four,': 5, 'in': 6, 'not': 7, 'or': 8, 'paragraph.': 9, 'sentences': 10, 'should': 11, 'taught': 12, 'teacher': 13, 'that': 14, 'there': 15, 'they': 16, 'three,': 17, 'when': 18, 'wrong': 19, 'you': 20, 'your': 21, 'aim': 22, 'grammar': 23, 'hard-and-fast': 24, 'however,': 25, 'impart': 26, 'important': 27, 'is': 28, 'it': 29, 'of': 30, 'rule': 31, 'teaching': 32, 'the': 33, 'this': 34, 'to': 35, 'understand,': 36, 'was': 37, 'an': 38, 'authoritative-but-dusty': 39, 'book.': 40, 'drawn': 41, 'from': 42, 'and': 43, 'effective.': 44, 'ideas': 45, 'must': 46, 'persuasive': 47, 'strategy': 48, 'supported': 49, 'teach': 50, 'true': 51, 'well': 52, 'about': 53, 'american': 54, 'by': 55, 'came': 56, 'culture.': 57, 'dogs': 58, 'domesticated': 59, 'has': 60, 'how': 61, 'humans': 62, 'insight': 63, 'native': 64, 'played': 65, 'provided': 66, 'recent': 67, 'research': 68, 'roles': 69, 'wealth': 70, '40,000': 71, 'ago.': 72, 'archaeological': 73, 'as': 74, 'been': 75, 'dna': 76, 'finds': 77, 'have': 78, 'long': 79, 'may': 80, 'on': 81, 'studies': 82, 'suggest': 83, 'years': 84, '12,000': 85, 'ago,': 86, 'america': 87, 'at': 88, 'eurasia,': 89, 'first': 90, 'least': 91, 'north': 92, 'them.': 93, 'with': 94, 'animal': 95, 'appear': 96, 'centuries': 97, 'companions': 98, 'early': 99, 'for': 100, 'highly': 101, 'hunter-gatherers': 102, 'only': 103, 'prized': 104, 'their': 105, 'were': 106, '16th': 107, 'century.': 108, 'continent': 109, 'horses': 110, 'no': 111, 'since': 112, 'until': 113}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4220e4-455e-40bc-96a1-412f03f9341c",
   "metadata": {},
   "source": [
    "## Let's create corpus object that contains the word id and its frequency in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd6cc0f-e1f4-4e7e-a13e-2c74b9219bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 2), (21, 1)], [(0, 1), (6, 1), (7, 1), (14, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 2), (36, 1), (37, 1)], [(38, 1), (39, 1), (40, 1), (41, 1), (42, 1)], [(1, 2), (14, 1), (20, 1), (21, 1), (22, 1), (30, 1), (33, 1), (34, 1), (35, 2), (37, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1)], [(0, 1), (1, 1), (6, 1), (16, 1), (30, 1), (33, 1), (35, 1), (43, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1)], [(14, 1), (55, 1), (58, 1), (59, 1), (62, 1), (71, 1), (72, 1), (73, 1), (74, 2), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1)], [(18, 1), (33, 1), (35, 1), (42, 1), (56, 2), (58, 1), (59, 1), (62, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1)], [(16, 1), (35, 1), (43, 1), (54, 1), (55, 1), (75, 1), (78, 1), (92, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1)], [(15, 1), (33, 2), (81, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1)]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in texts]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "057fbc72-cc8e-4096-8f6e-5a5043f65af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('a', 1), ('be', 1), ('childhood', 1), ('did', 1), ('five', 1), ('four,', 1), ('in', 1), ('not', 1), ('or', 2), ('paragraph.', 1), ('sentences', 1), ('should', 1), ('taught', 1), ('teacher', 1), ('that', 1), ('there', 1), ('they', 1), ('three,', 1), ('when', 1), ('wrong', 1), ('you', 2), ('your', 1)], [('a', 1), ('in', 1), ('not', 1), ('that', 1), ('aim', 1), ('grammar', 1), ('hard-and-fast', 1), ('however,', 1), ('impart', 1), ('important', 1), ('is', 1), ('it', 1), ('of', 1), ('rule', 1), ('teaching', 1), ('the', 1), ('this', 1), ('to', 2), ('understand,', 1), ('was', 1)], [('an', 1), ('authoritative-but-dusty', 1), ('book.', 1), ('drawn', 1), ('from', 1)], [('be', 2), ('that', 1), ('you', 1), ('your', 1), ('aim', 1), ('of', 1), ('the', 1), ('this', 1), ('to', 2), ('was', 1), ('and', 1), ('effective.', 1), ('ideas', 1), ('must', 1), ('persuasive', 1), ('strategy', 1), ('supported', 1), ('teach', 1), ('true', 1), ('well', 1)], [('a', 1), ('be', 1), ('in', 1), ('they', 1), ('of', 1), ('the', 1), ('to', 1), ('and', 1), ('about', 1), ('american', 1), ('by', 1), ('came', 1), ('culture.', 1), ('dogs', 1), ('domesticated', 1), ('has', 1), ('how', 1), ('humans', 1), ('insight', 1), ('native', 1), ('played', 1), ('provided', 1), ('recent', 1), ('research', 1), ('roles', 1), ('wealth', 1)], [('that', 1), ('by', 1), ('dogs', 1), ('domesticated', 1), ('humans', 1), ('40,000', 1), ('ago.', 1), ('archaeological', 1), ('as', 2), ('been', 1), ('dna', 1), ('finds', 1), ('have', 1), ('long', 1), ('may', 1), ('on', 1), ('studies', 1), ('suggest', 1), ('years', 1)], [('when', 1), ('the', 1), ('to', 1), ('from', 1), ('came', 2), ('dogs', 1), ('domesticated', 1), ('humans', 1), ('years', 1), ('12,000', 1), ('ago,', 1), ('america', 1), ('at', 1), ('eurasia,', 1), ('first', 1), ('least', 1), ('north', 1), ('them.', 1), ('with', 1)], [('they', 1), ('to', 1), ('and', 1), ('american', 1), ('by', 1), ('been', 1), ('have', 1), ('north', 1), ('animal', 1), ('appear', 1), ('centuries', 1), ('companions', 1), ('early', 1), ('for', 1), ('highly', 1), ('hunter-gatherers', 1), ('only', 1), ('prized', 1), ('their', 1), ('were', 1)], [('there', 1), ('the', 2), ('on', 1), ('were', 1), ('16th', 1), ('century.', 1), ('continent', 1), ('horses', 1), ('no', 1), ('since', 1), ('until', 1)]]\n"
     ]
    }
   ],
   "source": [
    "word_counts = [[(dictionary[id], count) for id, count in line] for line in corpus]\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237afc0d-efff-4ff2-95bd-d7c474146b52",
   "metadata": {},
   "source": [
    "## In paragraphs, certain words always tend to occur in pairs (bigram) or in groups of threes (trigram). Because the two words combined together form the actual entity. For example: The word ‘machine’ refers a device or a gadget or apparatus and the word ‘learning’ can refer to acquisition of knowledge or skills . But combining them, ‘Machine Learning’, refers to something completely different.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63eb23f5-a12d-4875-88ea-bdd2eaa33d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50315e32-fe29-4500-ab54-fb2de54d9e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 31.68MB file will be downloaded\n",
    "dataset = api.load(\"text8\")\n",
    "dataset = [wd for wd in dataset]\n",
    "\n",
    "dct = corpora.Dictionary(dataset)\n",
    "corpus = [dct.doc2bow(line) for line in dataset]\n",
    "\n",
    "# Build the bigram models\n",
    "# Words below min_count frequency are ignored\n",
    "# Threshold represents a threshold for forming the phrases (higher means fewer phrases). \n",
    "# A phrase of words a and b is accepted if (cnt(a, b) - min_count) * N / (cnt(a) * cnt(b)) > threshold, where N is the total vocabulary size.\n",
    "bigram = gensim.models.phrases.Phrases(dataset, min_count=3, threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc8db9ea-d9ab-4d28-83ac-002554b97cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['study',\n",
       " 'the',\n",
       " 'surface',\n",
       " 'of',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'astronomy',\n",
       " 'is',\n",
       " 'generally_thought',\n",
       " 'to',\n",
       " 'have_begun',\n",
       " 'in',\n",
       " 'ancient',\n",
       " 'babylon',\n",
       " 'by',\n",
       " 'the',\n",
       " 'persian',\n",
       " 'zoroastrian',\n",
       " 'priests',\n",
       " 'the',\n",
       " 'magi',\n",
       " 'recent_studies',\n",
       " 'of',\n",
       " 'babylonian',\n",
       " 'records',\n",
       " 'have',\n",
       " 'shown',\n",
       " 'them',\n",
       " 'to',\n",
       " 'be',\n",
       " 'extremely_accurate',\n",
       " 'for',\n",
       " 'the',\n",
       " 'ancient',\n",
       " 'night_sky',\n",
       " 'following',\n",
       " 'the',\n",
       " 'babylonians',\n",
       " 'the',\n",
       " 'egyptians',\n",
       " 'also',\n",
       " 'had',\n",
       " 'an',\n",
       " 'emphasis_on',\n",
       " 'observations',\n",
       " 'of',\n",
       " 'the',\n",
       " 'sky',\n",
       " 'mixtures',\n",
       " 'of']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram[dataset[10]][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e563eebd-e1c4-43a3-8d42-66b9e2e1de89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit (conda)",
   "language": "python",
   "name": "python3613jvsc74a57bd02310b29dc1dc5ec74e8e4640e6baf2a33fc38d279132d436ab31258a1c4e1d90"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
